[TOC]

```
1. 本书《算法第四版》从2016.12.18开始阅读。
2. 本书所有练习代码都在GitHub上维护。地址：https://github.com/FanZhouk/Algorithm.git
3. 笔记使用Typora编写，在GitHub上会有目录和公式无法显示的问题。
4. 一些章节的目录顺序可能与书中的顺序稍有不同。
5. 若Eclipse等编辑器无法导入项目，需进行以下配置：windows–>Preferences–>Team–>Git–>Configuration–>User Settings，点击Add Entry添加键值对：Key="http.sslVerify",Value="false"。
```

# 第一章 基础

## 1.1 基础编程模型

## 1.2 数据抽象

抽象数据类型（ADT），开发中会碰到很多种抽象数据类型，涉及数学的比如整数、浮点数，涉及数据结构的比如数组、链表，与现实相关的比如日期（Date）等。

使用抽象数据类型的好处在于，我们使用这些ADT的时候是面向接口编程的，即我们可以将一种实现改变为另一种实现，而无需改变调用部分的代码。

## 1.3 背包、队列和栈

**背包**：是一种不支持删除元素的集合数据类型，它的目的就是帮助用例收集元素，并迭代遍历所有收集到的元素。使用背包就说明，元素的处理顺序不重要。可以想象一个背包，往里面放了n个小球，取出的时候手伸进背包，球是随机取出来的。通常用于计算平均值等对元素顺序没有要求的算法。

**队列**：一种先进先出的数据结构，代码见：`com.fzk.adt.Queue<E>`

**栈**：一种后进先出的数据结构，代码见：`com.fzk.adt.Stack<E>`

注，这里队列和栈的代码都是利用链表实现的。

 

应用：

* **算术表达式求值**：利用Dijkstra的双栈求值法。一个栈存储操作符，另一个栈存储计算的数字。

  遇到数字就压入数字栈，遇到四则运算符就压入操作符栈，遇到右括号就弹出两个数字栈栈顶元素和操作符栈顶元素，进行计算，并压入栈。直到字符串读取完成，停止循环。

  * 代码见：`com.fzk.util.MathUtil.evaluate(String)` 

* **后序表达式求值**：只需要使用一个栈即可，它用于存储计算过程中产生的数字。

  遇到数字就压入栈，遇到操作符就弹栈计算结果并重新压入栈，这样计算结束后最后剩余的一个数字就是表达式的结果。前序表达式原理类似。

  * 代码见： `com.fzk.util.MathUtil.evaluatePostfix(String)` 

* **链表**：本处实现了双向循环链表，与 `java.util.LinkedList<E>` 类似。实现链表之后，可以轻松的利用链表来实现背包、队列、堆栈的功能。

  * 单链表的就地翻转：实现方法是，首先取到首结点和尾结点的指针，尾结点固定，首结点从头到尾移动，依次插入尾结点的下一位。但这里说的插入，仅仅是通过改变next域来实现的，并不是创建新的结点。
  * 代码见：链表：`com.fzk.adt.LinkedList<E>` ，单链表翻转： `com.fzk.adt.LinkedQueue.reverse()` 、 `com.fzk.adt.LinkedStack.reverse()` 



tips：

* **宽接口与窄接口**：在接口设计（尤其是底层代码，如数据结构的接口设计）中，我们经常会遇到“要不要这个功能”的问题。这里我们要尽量遵守一个原则：根据类的功能定义接口。不要想到什么功能就一股脑往里面添加。

  比如设计jdk中的Stack，只需要实现堆栈的基本功能就可以了，但要是为了类的“多功能”，而实现了向栈底插入元素等队列的功能，这个类就会显得不伦不类。

  我们尽量保持窄接口，是因为窄接口可以让我们清楚地明白类的功能和特性（如栈就只有入栈和出栈），限制用例的行为，使用例代码更加易懂。还有一点就是性能问题：宽接口无法保证高效的实现所有接口。

* **foreach语句**：Java中的foreach语句（如 `for (String s : list)` ），执行时会隐式创建一个迭代器，因此遍历的对象必须是实现Iterable接口的，即拥有一个返回Iterator迭代器的方法。


## 1.4 算法分析

要评价一个算法的好坏，主要看它的执行时间。比如ThreeSum问题，最容易想到的是进行三遍循环（当然还有更好的方法），那么它的运行时间是~$aN^3$。其中“~”表示与哪个数量级近似，小于该数量级的则省略；$a$表示常数，这个常数取决于计算机性能。

进行算法分析的一个好处在于，它把“算法”和“算法的实现”隔离开来。也就是说，只要算法确定了，不论是在微型计算机还是在大型计算机上，它们的算法复杂度是相同的。

特别注意，$lgN$是以2为底，而不是数学上习惯的以10为底数，因为在分析复杂度只需要精确到数量级，对于log来说底数是任何大于1的有理数都没有差别，以2为底数只是方便计算而已。

常见的增长数量级：

| 描述     | 数量级    | 算法   |   举例 |
| :----- | ------ | :--- | ---: |
| 常数级别   | $1$    | 普通语句 | 堆栈操作 |
| 对数级别   | $lgN$  | 二分策略 | 二分查找 |
| 线性级别   | $N$    | 单次循环 | 元素搜索 |
| 线性对数级别 | $NlgN$ | 分治策略 | 归并排序 |
| 平方级别   | $N^2$  | 双层循环 | 插入排序 |
| 立方级别   | $N^3$  | 三层循环 |    - |
| 指数级别   | $2^N$  | 穷举查找 |    - |

 

一个小概念：下界。下界是指一个算法在最坏情况下的时间复杂度。比如2-sum问题的下界就是$NlgN$，3-sum问题的下界是$N^2$，K-sum问题的下界是$N^{K-1}(K\geq3)$。

2-sum问题，代码见：`com.fzk.util.ArrayUtil.twoSum(int[], int)`

3-sum问题，代码见：`com.fzk.util.ArrayUtil.threeSum(int[], int)`

 

在估算算法性能时，有几点可能引起估算不准确：

* 大常数：比如对一个算法的估算是$2N^2+cN$，我们通常会简化为$N^2$。这里有一个隐藏的前提条件，即常数$c$很小。但当$c=10000$甚至更大时，显然模型就不准确了。
* 非决定性内循环。
* 指令时间：每条指令执行的时间不一定完全相同。
* 系统因素：与计算机性能、网络连接有关。
* 不分伯仲：程序在某些场景很快，在特殊场景下又很慢。
* 对输入的强烈依赖：最好情况下复杂度是常数级别，最坏就没边了。
* 多个问题参量。

## 1.5 案例研究：union-find算法

本节主要讲解并查集数据结构。并查集（Union-Find），核心功能是存储 $n$ 个结点的图结构，并存储其中的结点连通信息。

例如，并查集（图）中一共有5个结点，我们使用5个元素的数组 `int[] id = [0, 1, 2, 3, 4]` 来存储，这个数组就表示“含有5个独立连通分量的并查集数据结构”。利用这个数组还可以存储任意两个结点的连接关系。记录连接信息的算法有三种，分别见下面三小节。

并查集的API如下，其中三种算法的区别就在于 `union(int, int)` 方法和 `find(int)` 方法。

```java
public class UnitFind {
	public int[] id; // 分量ID，用于存储每个结点所属的连通分量
	private int count; // 连通分量数量

	// 在p和q之间创建一条连接
	public void union(int p, int q);
	// 获取p所在分量的标识符
	public int find(int p);
	// 判断p和q是否相连
	public boolean connected(int p, int q);
	// 获取连通分量的数量
	public int count();
}
```

### 1.5.1 quick-find算法

**算法原理**：quick-find是并查集中最容易理解的一种算法，它的特点是，数组 `id[]` 中，同一个连通分量的所有触点的值全部相同，换句话说，当且仅当 `id[p]==id[q]` 时，p和q是连通的。

比如数组 `[2, 2, 2, 2, 2, 5, 6, 9, 9, 9]` ，共有10个结点，其中第1~5个结点是互相连通的，第8~10个结点也是互相连通的。

根据以上实例我们可以看出，同一个连通分量的数值一定是相同的，因此一个分量里任意一个触点的数值就可以作为分量的标识符，即每个分量都是由它的触点之一表示的。这种表示方式的优点是，连通信息一目了然，只要数字相同的，就属于同一个连通分量。

`find(int)` 和 `union(int, int)` 方法的逻辑也很简单。`find(int)` 方法直接获取数组上对应的数字即可，`union(int, int)` 方法首先获取到两个结点所在的分量标识，若不在同一分量中，则遍历整个数组，把两个分量的数值统一即可。

**算法分析**：算法无法处理大型问题，因为每一次执行`union(int, int)` 方法都要扫描整个数组。将规模为 $n$ 的并查集归并为最后只有一个连通分量，时间复杂度为 $N^2$ ，平方级别。

代码见： `com.fzk.adt.unionFind.QuickFindUF` 

### 1.5.2 quick-union算法

**算法原理**：quick-union算法与quick-find的区别的实质在于，该算法并不是所有连通分量都共用一个标识，而是每个连通分量都形成了一棵树，只有在树的根结点处，才满足 `id[i] == i` ，即位置和数值相同。而对于树的其他结点，它们的数值会指向数组中的其他位置，如果这个位置是根结点，那么一棵树就形成了，否则可以继续向上追溯，直到根据一个叶子结点找到树的根结点。

比如 `[3, 5, 5, 4, 5, 5, 9, 1, 2, 8]` 这个数组，索引为0的位置指向了3，3指向了4，4又指向了5，因此该并查集中的树根结点就是5。而且还可以看出，整个并查集只有一个根结点，即10个结点都属于同一连通分量。

`find(int)` 方法：只要一直向上找到满足 `id[i] == i` 的条件的结点即可，这个结点就是传入结点的根结点；

`union(int, int)` 方法：分别找到两个结点的根结点，然后把其中一个根结点的数值变为另一个根结点，即把一棵树转换为另一棵树的子树。

**算法分析**：是quick-find算法的一种改进，最佳情况下，时间复杂度是线性级别，最坏情况下是平方级别。

代码见： `com.fzk.adt.unionFind.QuickUnionUF` 

### 1.5.3 加权quick-union算法

**算法原理**：是对quick-union算法的进一步优化。区别在于，该算法会记录下每一棵树的根结点的树的大小，根据这个大小，当执行union方法时，会保证永远将小树挂在大树上，这样可以保证树的深度尽量小，即find方法需要访问数组的次数尽量少。

`find(int)` 方法：与quick-union算法相同；

`union(int, int)` 方法：找到两个结点的根结点，将较小的树根挂在较大的树根下。

**算法分析**：利用加权quick-union算法，可以在最坏情况下的时间复杂度为 $lgN$ 。

代码见： `com.fzk.adt.unionFind.WeightedQuickUnionUF` 

注：书中的三个测试文件tinyUF.txt、mediumUF.txt、largeUF.txt可以在 [这里](http://algs4.cs.princeton.edu/15uf/) 下载到。

# 第二章 排序

## 2.1 初级排序算法

### 2.1.1 游戏规则

评判一种排序算法的优劣，可以通过以下两个方面：

1. 运行时间

排序成本模型：在研究排序算法时，我们需要计算“比较”和“交换”的数量。对于不交换元素的算法，我们会计算“访问数组”的次数。

2. 额外的内存使用

根据是否需要占用额外存储空间，可以将排序算法分为：就地排序算法和其他排序算法。

### 2.1.2 选择排序

**主要思想**：循环数组，找出最小的，与第一个元素交换（当第一个元素就是最小的时候，与自己交换）；找出第二小的，与第二个元素交换...直到循环整个数组。

**复杂度**：~$\frac{N^2}{2}$ 

**算法特点**：

* 运行时间与输入无关。即无论输入是有序数组还是乱序数组，比较次数是完全一样的；
* 复杂度较高；
* 是一种就地排序算法。

代码见：`com.fzk.util.SortUtil.selectionSort(int[])`

### 2.1.3 插入排序

**主要思想**：把数组分为两个区域：左边为有序区，右边为无序区。循环数组，每次取无序区的第一个，插入到有序区中的相应位置上去，比它大的元素各后移一位。直到整个数组都变为有序区为止。

**复杂度**：

* 最好情况下（输入数组严格有序），需要$N-1$次比较，$0$次插入；
* 最坏情况下（输入数组严格逆序），需要~$\frac{N^2}{2}$次比较，~$\frac{N^2}{2}$次插入；
* 平均情况下，需要~$\frac{N^2}{4}$次比较，~$\frac{N^2}{4}$次插入。



**算法特点**：

* 输入数组的好坏对算法效率影响很大；
* 适用于部分有序的数组（每个元素离它的最终位置都不远）；
* 是一种就地排序算法。



代码见：`com.fzk.util.SortUtil.insertionSort(int[])`

### 2.1.4 希尔排序

**主要思想**：多次插入排序。设定步长h，第一步使数组中任意间隔为h的元素都是有序的，形成一个h有序数组。下一步缩小步长为h/2，使数组成为一个h/2有序数组...h最后为1，对整个数组进行插入排序。

 

希尔排序是插入排序的改良版。插入排序只能一个一个元素的移动，这样效率会很慢。

 

第一步，分组。希尔排序会首先设定一个“步长”h，把索引为0,h,2h...这些元素看做一组，把索引为1,h+1,2h+1...这些元素也看做一组，以此类推。如下图所示：

![wpsB0E6.tmp.jpg](https://ooo.0o0.ooo/2017/01/01/5868fa84b255d.jpg) 

这张图表示，当数组长度为10，步长h为5时的分组情景。共分为5组（ABCDE组）。

第二步，组内排序。在每组内分别进行插入排序，如上图进行一轮组内排序后得到结果如下图。希尔排序相比直接插入排序的效率提升极大，关键就在这一步组内排序！

![wpsB0E7.tmp.jpg](https://ooo.0o0.ooo/2017/01/01/5868fa84c707b.jpg) 

第三步，缩小步长。缩小步长后重复组内排序，直到步长变为1。一般缩小步长的算法可以直接将原步长除以2。上图继续缩小步长为2，则下一次的组内排序变为下图：

![wpsB0E8.tmp.jpg](https://ooo.0o0.ooo/2017/01/01/5868fa84c9d23.jpg) 

继续缩小步长为1，如下图：

![wpsB0F8.tmp.jpg](https://ooo.0o0.ooo/2017/01/01/5868fc7845265.jpg) 

进行完步长h为1的时候，希尔排序宣告完成。

 

**算法分析**：希尔排序的关键就在于，组内排序！传统的插入排序要向有序区中间插入一个数字，只能把大于插入数字的所有元素都往后移动一位。这样太慢了！

希尔排序的做法是，首先分组，进行组内排序。第一次组内排序不要求元素一定在最终的位置上，但能保证进行一次组内排序后，每个元素都向最终的位置靠近了！

这样进行多轮组内排序，就可以完成整个算法的排序。

 

希尔排序选择插入排序做组内排序算法的原因在于，插入排序有一个极大的优点：输入数组越是接近有序，排序速度越快！最好情况可达到线性的复杂度。

而不断组内排序，就构造了一个这样的环境：数组变得越来越接近有序。这样可以让插入排序发挥它的优势。

 

> 通过提升速度来解决其他方式无法解决的问题，是研究算法的设计和性能的主要原因之一。
>
> ——名人名言

 

**复杂度**：当使用的递增序列为$3h+1$时（即1,4,13,40,121...，见代码），最坏情况下，比较次数与~$N^{\frac{3}{2}}$成正比。

使用不同的递增序列，比较的次数会各不相同，但都会小于直接插入排序的~$N^{2}$。

 

代码见：`com.fzk.util.SortUtil.shellSort(int[])`

参考链接：[白话经典算法：希尔排序](http://blog.csdn.net/morewindows/article/details/6668714)

## 2.2 归并排序

### 2.2.1 自顶向下的归并

**主要思想**：递归，合并。

假如原数组长度为$n$，先排序左半部分（$0$ ~ $\frac n 2$），再排序右半部分（$\frac n 2+1$ ~ $n-1$）。最后将这两个子数组合并。

其中两个对子数组的排序用的也是归并排序，这就体现出递归的思想。

归并排序还包含了一个重要的思想：分治思想。

**复杂度**：时间复杂度：假设原数组长度为$N$，那么需要递归$n=lgN$次。想象成树形结构，那么这棵树有$n$层。则第k层共有$2^k$个子数组，每一个子数组的长度是$2^{n-k}$，则第$k$层要比较$2^{n-k} \times 2^k=2^n$次。则每层比较的次数与层数无关。那么一共$n$ 层，共需要比较$n \times 2^n=NlgN$次。因此归并排序的时间复杂度为~$NlgN$。

空间复杂度：需要一个与原数组长度相同的辅助数组，因此空间复杂度是~$N$。

**算法优化**：

1. 用插入排序代替小数组的归并。由于归并排序是一种递归算法，因此对于递归到很小的数组（比如长度为2的子数组）时仍然使用归并排序，这样对效率略有损耗。这时对小数组采用简单的插入排序，效率会略有提升。
2. 在融合之前，判断一下数组是否已经有序。当满足条件`arr[mid] <= arr[mid+1]`的条件时，说明数组已经有序，不需要融合了。因此添加这个判断后，对于严格有序的（子）数组，可以达到线性级别的复杂度。
3. 不要复制到辅助数组。由于每次融合都经历了“从原数组融合到辅助数组，再从辅助数组复制回原数组”的过程，这样比较浪费时间。我们可以在递归调用的每个层次交换原数组和辅助数组的角色，以节省复制所有元素的时间。
4. 多路归并。普通的归并排序是把数组分为两份，分别对每个子数组归并，然后再融合。现在可以首先分成三份，进行三向归并。


代码见：`com.fzk.util.SortUtil.mergeSort(int[])` 

### 2.2.2 自底向上的归并

**主要思想**：先两两合并，再四四合并，再八八合并，直至整个数组合并完成。

假设原数组长度为$n$，首先将每一个单独的数字看做一个长度为$1$的归并单元，两两归并，保证 `arr[0] < arr[1], arr[2] < arr[3]` 等等。再进行四四归并，即将第一步排序得到的数组，每两个元素看成一组已经排好序的归并单元，将两个归并单元进行归并，保证每四个元素都是有序的。这样直至整个数组归并完成。

**适用场景**：适用于对链表的排序。

代码见：`com.fzk.util.SortUtil.mergeSort2(int[])` 

**一些题外话**：数学可证，没有任何基于比较的算法能够保证使用少于~$NlgN$次比较将长度为$N$的数组排序。而又可以证明，对于任意长度为$N$的数组，归并排序在最差情况下需要访问数组$6NlgN$次。基于以上两点，可以得出结论：归并排序是一种渐进最优的基于比较排序的算法。

 所以，不要费尽心思寻找小于$NlgN$的比较排序算法啦，找不到的！

## 2.3 快速排序

### 2.3.1 普通快速排序

**主要思想**：分治策略。

选择一个基准数（pivot），第一趟循环让所有小于pivot的值放在左边，大于pivot的值放在右边，而pivot放在最终位置。

接着递归排序左边、右边即可。

**算法优化**：

1. 切换到插入排序。大多数的递归算法，改进算法性能有以下原则：对于小数组，递归排序要比直接插入排序慢。因此当要排序数组很小的时候，可以替换为插入排序。这样第一提高了速度，第二减少了递归深度。
2. 三取样切分。由于一开始的基准数（pivot）是随意取的，因此很有可能取到最大或最小值。以下办法可以改进：在数组中任意取3个数，并计算它们的中位数，把这个中位数当成基准数。这种方法称为三取样切分。
3. 熵最优的排序。实际中经常会出现含有大量重复元素的数组。这是，一个元素全部重复的子数组就不需要继续排序了。

代码见： `com.fzk.util.SortUtil.quickSort(int[])` 

### 2.3.2 三向切分的快速排序

**主要思想**：一次遍历，将所有等于pivot的值聚在一起。

普通的快排仅仅将数组分为两部分：小于pivot的和大于pivot的（当然，还有pivot本身）；而三向切分的快排会将数组分为三部分：小于pivot、等于pivot、大于pivot。

在一次循环中，会维护三个指针，分别是： `l` 为从左向右的指针， `h` 为从右向左的指针， `i` 在 `l` 和 `h` 中间，左边都是等于pivot，右边都是未知部分，如下图所示：

![三向切分快速排序过程图](https://ooo.0o0.ooo/2017/05/19/591dcee481c45.png)

这样进行一次循环之后，可以保证将所有等于pivot的值聚集在一起，并且将小于pivot的放在左边，将大于pivot的放在右边。继续迭代左边和右边进行排序即可。

**适用场景**：适用于存在大量重复主键的数组排序，如对大量人员信息根据性别排序、根据年龄排序等。

代码见： `com.fzk.util.SortUtil.quickSort3way(int[])` 

## 2.4 优先队列

### 2.4.1 二叉堆

一种数据结构：二叉堆。

二叉堆是一种顺序存储结构，内部用数组存储，但通过简单的运算可以表示成完全二叉树的结构。

比如根结点在位置$1$（索引为0的位置为空），那么索引为$k$的元素的左结点在$2k$，右结点在$2k+1$，父结点在$\lfloor \frac k 2 \rfloor$。（这里取根结点位置是$1$，主要是为了方便子结点和父结点的运算）。

二叉堆可分为最大堆和最小堆。若将二叉堆转化为树形结构，若任何一个父结点大于它的两个子结点（如果有的话），那么它就是最大堆；若任何一个父结点小于它的两个子结点，那么就是最小堆。

以最大堆为例，存储结构如图所示：这是一个存储了10个元素的二叉堆，占用数组长度为11，右图为位置k的父结点和左右子结点位置。可以看出这是一棵完全二叉树结构。

![](https://ooo.0o0.ooo/2017/01/05/586da589b60fc.png)

### 2.4.2 优先队列

**ADT简介**：以最大优先队列为例，最大优先队列可以存储一组对象数据，主要提供以下方法：

1. 增加一个元素；
2. 删除并返回最大元素。

若用普通的顺序结构或链表结构，也能完成这样的需求，但要么插入的复杂度是~$N$，要么删除的复杂度是~$N$。

现在使用优先队列，可以使插入和删除操作的复杂度都变为$lgN$。

**主要思想**：要实现优先队列，主要实现三个方法：1）增加一个元素；2）删除最大元素；3）给二叉堆扩容。这里以最大优先队列为例，其中二叉堆是最大堆。

1. 增加一个元素：首先将这个元素添加到最后（size所处的位置，不是真正的数组最后的位置），然后将这个元素“上浮”，即只要遇到比它大的父结点，就交换，直到小于它的父结点为止。
2. 删除最大元素：由于最大堆的第一个元素就是最大元素，因此只需要删除第一个元素即可。首先用队列末尾的元素替换第一个元素，然后将刚换上来的这个元素“下沉”，即：不断的将这个元素与它的两个子结点中较大的一个交换，直到两个子结点都比它小，或没有子结点为止。
3. 给二叉堆扩容：扩容策略可以有多种，直接提升一倍容量，提升为1.5倍容量等等均可。复制数组直接用`java.util.Arrays.copyOf(T[], int)`即可。


**复杂度**：对于一个含有$N$个元素的二叉堆优先队列，插入元素操作最大需要比较$lgN+1$次，删除最大元素操作最多需要比较$2lgN$次。

**算法优化**：

1. 除了二叉堆，我们还可以利用三叉堆......任意叉堆。以三叉堆实现的最大堆为例，那么一个结点必须要大于它的三个子结点。其中位置$k$的结点的三个子结点分别为：$3k-1$，$3k$，$3k+1$，它的父结点是$\lfloor \frac{k+1}{3} \rfloor$。
2. 索引优先队列。在最大堆实现的优先队列中，一个元素一旦插入，就不能改变，因为改变会影响到这个元素的排序。如果我们有改变元素的需求，可以在插入每个元素时，给这个元素关联一个索引，并用一个（多个）平行数组来存储索引。


**最小优先队列**：与最大优先队列几乎相同，唯一的区别在于上浮和下沉的算法。最大优先队列会把大数字上浮，把小数字下沉，而最小优先队列刚好相反，把小数字上浮，把大数字下沉。

代码见：

* 最大优先队列：`com.fzk.adt.MaxPriorityQueue<E>` 
* 最小优先队列：`com.fzk.adt.MinPriorityQueue<E>` 

### 2.4.3 堆排序

**主要思想**：首先将数组“堆化”，然后依次取出最大数字放在最后，并将剩余元素重新构建为最大堆。

堆排序的第一步，“堆化”数组，即将数组构建为一个最大堆。较容易想到的办法是，从前向后执行 `swim()` 方法，这样总能保证指针左侧的数组是一个最大堆。但这样效率较低，有一种更好的方法：从数组中间位置开始，向前遍历，不断执行 `sink()` 方法。这种方法的高效之处在于，数组中间位置之前的所有元素，都有左右两个子结点（中间位置元素可能有一个）。

其实这个中间位置，指的就是数组最后一个元素的父结点所在的位置，这个位置能保证至少有一个子结点，而这个位置之前的所有元素，都有左右两个结点。因此将前一半元素进行下沉，可以节省一半的时间。如图所示：

![堆化数组](https://ooo.0o0.ooo/2017/05/24/59245dbfb3aaa.png)

假如数组长度 `arr.length = 10` ，则应该从 `parent(arr.length - 1)` 即索引为4处开始下沉，接着是3、2、1、0的下沉。这样就达到了只遍历一半数组，就将整个数组构建为最大堆的目的。

第二步较容易理解，每次取出堆的最大元素，与堆的最后一个元素交换，这样保证了这个堆的最后一个元素是最大的，此时立刻将换到最后的元素踢出最大堆（ `size--` 即可），并将换到最前的元素下沉，以形成一个新的最大堆。这样循环下去，就可以保证每次取出的都是当前堆中的最大元素，即从小到大依次排列在堆的后面。等到遍历结束，则排序完成。

**复杂度**：比较次数为 $2NlgN+2N$ ，交换次数为 $N/2$ 。

代码见： `com.fzk.util.SortUtil.heapSort(int[])` 

注：该代码中，数组索引为0的位置也被利用了，因此获取父子结点索引的方法略有区别。

## 2.5 应用

### 2.5.1 排序的特性

**指针排序**：在java中，我们不需要指定直接操作数据（值传递）还是操作数据的指针（引用传递）。java中指针操作是隐式的，只有在传递8中基本数据类型数据的时候，才是传值的（String类型也类似于值传递，即不会被改变），其他的对象在传递时都是传引用。

因此当我们排序的对象是对象时，我们交换的其实不是对象本身，只是对象的引用。因此这是一种“廉价的交换”。

**排序的稳定性**：如果一个排序算法能够保留数组中重复元素的相对位置，则被称为是稳定的。

比如我们要对订单排序，规则是根据时间和地点同时排序。可以进行的办法是，先对时间排序后，将这个结果再对地点排序。这就要求排序算法必须是稳定的，否则对地点排序后无法保证对时间也是排好序的。

稳定排序算法常被用于多主键共同排序的场景中。

本章的算法中，插入排序和归并排序是稳定的，选择排序、希尔排序、快速排序和堆排序都是不稳定的。

### 2.5.2 排序算法的比较

 **排序比较**：各种排序算法的性能特点如下表。

| 算法     | 是否稳定 | 是否就地 | 时间复杂度      | 空间复杂度 |
| :----- | :--- | :--- | :--------- | ----: |
| 选择排序   | 不稳定  | 就地   | $N^2$      |   $1$ |
| 插入排序   | 稳定   | 就地   | $N$~$N^2$  |   $1$ |
| 希尔排序   | 不稳定  | 就地   | $NlgN$     |   $1$ |
| 快速排序   | 不稳定  | 就地   | $NlgN$     | $lgN$ |
| 三向快速排序 | 不稳定  | 就地   | $N$~$NlgN$ | $lgN$ |
| 归并排序   | 稳定   | 非就地  | $NlgN$     |   $N$ |
| 堆排序    | 不稳定  | 就地   | $NlgN$     |   $1$ |

### 2.5.3 问题的规约

 规约指的是为解决某个问题而发明的算法正好可以用来解决另一种问题。对于许多算法问题，实际上都可以规约为排序的问题。

 **找出重复元素**：

若暴力解决，需要循环两遍数组，在$N^2$时间才能完成。

更好的方法是先排个序，然后找相邻重复的就可以啦！代码太简单就不写了。。。

**求中位数**：

若暴力解决，需要先排个序，然后取中间位置的元素，在$NlgN$时间内才能完成。

更好的方法是，利用快排中的`partition()`方法。这个方法的作用是将一个元素放在正确的位置上，且左边的都比它小，右边的都比它大。

首先随便取一个元素，把它放到正确位置上，要是这个位置在中间偏右，就在左边找；要是偏左，就在右边找。直到这个正确位置刚好等于中间位置。返回这个中间位置的元素即可。

代码见：`com.fzk.util.ArrayUtil.findMedian(int[])` 

//TODO: 练习25.32 A*算法

# 第三章 查找

## 3.1 符号表

**ADT简介**：符号表是指一种存储一组键值对的抽象数据类型。实现符号表关键要实现三个方法：1）`put(K,V)`插入键值对；2）`get(K)`根据键查找值；3）`remove(K)`根据键删除键值对。

**无序链表的实现**：即最简单的单链表。插入、查找和删除操作的复杂度都是线性级别。

**有序数组的实现**：使用一对平行数组存储。一个存储键，一个存储值。

关键在于实现`rank()`方法，它返回表中小于给定键的数量。有了它，查找、插入、删除都可以很容易地实现。而由于数组本身是有序的，使用二分查找就可以实现`rank()`方法。

**二分查找分析**：对于有序数组，二分查找的复杂度是对数级别的，最多需要$lgN+1$次比较就可以确定结果，无论是否找到。证明过程挺好的，可以多看。见第三章命题B。

## 3.2 二叉查找树

**ADT简介**：一棵二叉查找树（BST, Binary Search Tree）是一棵二叉树，其中每个结点都含有一个Comparable的键（以及相关联的值），且每个结点的键都大于其左子树中任意结点的键而小于右子树任意结点的键。即左 < 中 < 右。

在由$N$个随机键构造的二叉查找树中，查找命中平均所需的比较次数为~$2lnN$（约为$1.39lgN$）。

### 3.2.1 有父指针的二叉查找树

所谓有父指针，即每个树结点对象中存储三个指针：左右子结点以及父结点的指针。有几个算法是比较重要的，主要思路如下：

* <u>添加一个键值对</u>：首先从根结点开始，找到要添加或修改的位置，要是键冲突直接改变即可。不冲突的话，比这个键小就放在左边，大就放在右边。


* <u>根据键获取值</u>：从根结点开始寻找，小于当前访问结点的，就在左边找，否则就在右边找。

* <u>根据键移除键值对</u>：找到这个键所对应的结点后，分三种情况：1）它是叶子结点；2）只有一个子结点；3）有两个子结点。

  1）是叶子结点：删除叶子结点是最容易的，看它在父结点的左边还是右边，对应的指针设为空就行了；

  2）有一个子结点：也比较容易，把它的父结点和它仅有的那一个结点关联起来就好了；

  3）有两个子结点：较复杂。取它的前趋（后继也可），替换掉这个结点，然后删除这个前趋（后继）。由于前趋或后继结点最多只能有一个子结点，因此删除前趋（后继）的操作递归地可以交给1）和2）去做。

  注意：以上三种情况都要考虑是否是根结点，根结点的父结点为空，处理情况稍有不同。注意判断即可。


* <u>获取结点的前趋和后继</u>：获取前趋的方法是，若它有左子结点，那么前趋就是它左子结点的最右边一个。若没有左子结点，沿着它向上找，一旦发现它是右子结点，那么它的父结点就是原结点的前趋。后继类似。寻找前趋示意图如下。

![](https://ooo.0o0.ooo/2017/01/09/5873ac62ca3bb.png)

* <u>遍历</u>：遍历分为前序、中序、后序和层次遍历。前序是指“中-左-右”的顺序，中序是指“左-中-右”的顺序，后序是指“左-右-中”的顺序，这三个都可以利用递归轻松地写出来。层次遍历利用队列即可实现。



**性能分析**：

* 在一棵二叉查找树中，所有操作在最坏情况下所需的时间都和树的高度成正比。
* 以上几乎所有方法都可以写成递归形式，但当树的高度过大时，递归调用的栈的深度过大，可能会引发性能问题。

代码见：`com.fzk.adt.BinarySearchTree<K extends Comparable<K>, V>` 

### 3.2.2 无父指针的二叉查找树

所谓无父指针，即每个树结点对象中只存储左右子结点的指针，不存储其父结点的指针。由于上节中寻找前趋和后继结点都依赖于父指针，而删除操作又依赖于前趋和后继，因此删除相关方法的实现会有一些变化。

* 删除最大最小结点：使用递归完成。以删除最小键为例，若它的左结点为空，说明它本身就是最小键了，删除的逻辑是：将最小结点父结点的左指针，指向最小结点右指针指向的元素，这样就保证了没有任何指针指向原最小元素，则会被GC回收；若它的左结点非空，递归地删除左结点的最小键。删除操作如图所示：

  ![删除最小键](https://ooo.0o0.ooo/2017/05/30/592ce0f4ce658.png)

* 删除任意结点：思路与有父指针的树相似，同样是分成三种情况：无子结点、一个子结点和两个子结点。其中两个子结点的情况同样是用后继来替换，然后删除原有的后继（体现在代码中就是删除右结点的最小结点，可调用前面写好的方法，仅这一点与有父指针的树不同）。

## 3.3 平衡查找树

### 3.3.1 2-3查找树

#### 3.3.1.1 概念

一棵2-3查找树由任意个以下结点组成：

* 2-结点：与普通的二叉树结点相同，含有一个键和两个链接：左链接和右链接，左链接小于该键值，右链接大于该键值；
* 3-结点：含有两个键和三个链接：左链接都小于该结点中的两个键，中链接中的键介于该结点的两个键之间，右链接都大于该结点中的两个键。

下图展示了2-结点和3-结点，同时也是将0-9这10个数字依次插入2-3查找树的结果：

![2-3查找树](https://ooo.0o0.ooo/2017/05/30/592d4662624d4.png)

#### 3.3.1.2 树的构建

下面讲解如何构建一个2-3查找树：

* 查找：与普通二叉树类似，同样是小了就在左边找，大了就在右边找，区别在于如果查到了3-结点，还需要判断如果在两个键之间，则在中间的链接中找；
* 插入：
  * 若向2-结点中插入，则直接把它变成3-结点；
  * 若向3-结点中插入，先把它变成一个临时的“4-结点”，这时该结点中存储了3个键，把处于中间大小的键向上推，即插入它的父结点，然后把原有的3-结点分开，作为新的父结点的左右结点。这是一个递归的过程，如果父结点仍然是3-结点，则还要构建4-结点，继续向上推；
  * 若根结点本身就是一个3-结点，则构建成4-结点后，把中间大小的键提出来作为新的根结点，剩下的两个分别作为它的左右子结点，这就是分解根结点的过程。

#### 3.3.1.3 树的性质

* 全局平衡性：任意叶子结点到根结点的路径长度都是相等的，换句话说，如果把树中所有结点都看做是2-结点，树会变成一个满二叉树。这是因为3-结点具有“缓存”的功能，向2-结点中插入新的键时会构建出一个3-结点来缓存，而不是直接挂在2-结点下面，形成单独的子结点；
* 由下向上生长：2-3树的生长过程是由下向上的。总结一下树的生长方式：每次向树中插入结点，会首先查找到对应的叶子结点，如果叶子结点是3-结点，则取出中间的插入父结点，如此循环，直到找到一个2-结点，将其构造成新的3-结点为止；
* 高性能：一棵2-3树的大小为$N$，它的高度在$log_3N$至$log_2N$之间，查找和插入操作访问的结点数一定不超过$log_2N$。比如一棵大小为10亿的树的高度在19至30之间，查找和插入需要的访问次数不超过30。

### 3.3.2 红黑二叉查找树

红黑树是2-3树的一种实现方式。 

**2-3树与红黑树的转化**：

2-3树中，2-结点可以存放一个键，3-结点可以存放两个键。我们把3-结点中的两个键拆分成两个2-结点，并利用红链接将这两个结点连接起来，这样就完成了2-3树到普通二叉树的转化，这样形成的二叉树被称为红黑树。3-结点转化方式如下图所示：

![结点转化方式](https://ooo.0o0.ooo/2017/06/03/593263196603d.png)

从图中可以看出，如果我们将一棵红黑树中的红链接画平，那么所有的空链接到根结点的距离都将是相同的，这刚好是2-3树的性质相吻合。由此也可以得知，红黑树既是二叉查找树，也是2-3树，那么可以将两个算法的优点结合起来：二叉树高效的查找方法，与2-3树高效的平衡插入算法。

**红黑树的性质**：

* 红链接均为左链接；
* 没有任何一个结点同时和两个红链接相连——不会出现两个连续的红链接；
* 红黑树是完美黑色平衡的，即任意空链接到根结点路径上的黑链接数量都相同。

**红黑树的颜色表示**：

红黑树中每个结点都会有一个布尔型的颜色标识，要么是红色要么是黑色。这个颜色表示的是“指向该结点的链接的颜色”。如果一个结点中的颜色标识是红色，代表它父结点指向它的链接是红色的。

**红黑树的旋转**：

当插入一个新结点时，有可能会出现红色右链接或者两个连续的红链接的情况，但这与红黑树的性质相违背。通过旋转操作可以将这些错误修正过来。以左旋为例，旋转过程如图所示：

![左旋过程](https://ooo.0o0.ooo/2017/06/03/59326c621c1be.png)

**红黑树的插入操作**：//TODO

**红黑树的删除操作**：//TODO

**红黑树的性质**：

* 一棵大小为$N$的红黑树的高度不会超过$2lgN$。
* 一棵大小为$N$的红黑树中，根结点到任意结点的平均路径长度为~$1lgN$。
* 在一棵红黑树中，查找、插入、删除等所有操作在最坏情况下所需的时间都是对数级别的。

代码见： `com.fzk.adt.binarySearchTree.RedBlackBST<K, V>` 

//TODO：红黑树需要全部重新写一遍；

//TODO：模仿TreeMap写出带有父结点的红黑树实现。

## 3.4 散列表

散列表是一种使用数组来存储元素的数据结构，且能在线性时间内完成查找和插入操作。

散列表的关键算法有两个：一是计算散列函数，二是处理散列值冲突问题。

### 3.4.1 散列函数

要将无穷范围的元素（可能是整数、浮点数、字符串，也可能是任意对象）映射到有限范围内的数组中，我们需要一个函数来完成这个过程，即把任意对象转化为一个指定数组长度范围内的数字，这个函数就是散列函数。

一个优秀的散列方法需要满足三个条件：

* 一致性：相同的键必然产生相等的散列值；
* 高效性：计算高效；
* 均匀性：均匀地散列所有的键。

散列函数的功能分为如下两步：将任意对象转化为一个int值，并将该int值映射到数组长度范围内。

#### 3.4.1.1 将对象转化为int值

这一步的关键是保证均匀性，即尽可能包含对象中的全部信息。换句话说，这个对象改变了内部任意一个数据，计算出的值都要尽可能是不重复的。

比如一个日期对象，其中会包含年、月、日、时、分、秒、毫秒等信息，那么在设计散列函数的时候就需要保证，改变了其中任意一个值，得到的结果都尽可能不重复。字符串也类似，在计算时会用到每一个字符进行计算。可以参考jdk中各个类 `hashCode()` 方法的具体实现，它们都保证了这个特性。

比如下面一段代码是jdk1.8中计算字符串的哈希值的算法（去掉了缓存哈希值的部分）：

```java
public int hashCode() {
	int h = 0;
	char val[] = value;
	for (int i = 0; i < value.length; i++)
		h = 31 * h + val[i];
	return h;
}
```

其中 `value` 是当前字符串对应的字符数组，可以看出计算哈希值时，字符串的每一个字符都参与了计算。

#### 3.4.1.2 映射到数组范围内

最常用的方法是除留余数法。比如元素的哈希值为 `hash` ，数组的大小为 `M` ，则利用 `hash % M` 即可将任意哈希值散布在 0 到 M-1 的范围内。

需要注意的是，数组的大小尽量选取素数，否则会导致无法完全均摊到数组每个位置。比如数组的长度是100，那么哈希值中有效的数字仅仅是最后两位（ `1201 % 100` 和 `1301 % 100` 的结果是完全相同的），对于任意非素数都会有这个问题。

方法如下：

```java
private int hash(K key) {
	return (key.hashCode() & 0x7fffffff) % M;
}
```

这段代码会将符号位屏蔽（将一个32位整数变为一个31位非负整数），然后用除留余数法计算它除以M的余数。

#### 3.4.1.3 软缓存

将计算好的哈希值放在对象中维护起来，以免重复计算，这种方法称为软缓存。软缓存仅限于不可变的对象，比如jdk中的字符串类 `String` 。

下面是jdk中字符串类的缓存哈希值的逻辑：

```java
int hash;

public int hashCode() {
	int h = hash;
	if (h == 0 && value.length > 0) {
		// 仅当判断为没有计算过时，才会计算哈希值h...
		hash = h; // 将哈希值缓存起来
	}
	return h;
}
```

### 3.4.2 基于拉链法的散列表

jdk中的哈希表 `HashMap` 就是使用基于拉链法的散列表实现的。

实现方式：使用链表数组存储数据，每一个链表中的键都是哈希值相等的。当产生哈希值冲突时，把冲突的键放在一个链表中存储即可。

代码见： `com.fzk.adt.HashMap<K, V>` 

### 3.4.3 基于线性探测法的散列表

基于线性探测法的散列表也被称为开放地址的散列表。当碰撞发生时，直接检查散列表中的下一个位置，这样的线性探测可能产生以下三种结果：

* 命中，该位置的键和被查找的键相同，则直接改变这个位置的元素；
* 未命中，该位置还没有键，则直接赋值即可；
* 继续查找，该位置的键和被查找的键不同，则继续探测下一个位置。




















 # 备注

[一些奇形怪状的数学符号](http://blog.csdn.net/zcf1002797280/article/details/51289555)

[网络通畅但无法提交GitHub问题](http://www.cnblogs.com/yejiurui/p/3386393.html)
